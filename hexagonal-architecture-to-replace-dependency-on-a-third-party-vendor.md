# Hexagonal architecture to replace dependency on a third-party vendor
I was working with a no-code/low-code ITIL and BPA platform that the business was outgrowing. We had a few years left on the contract with the vendor for that software. I was the only full-time developer on a small IT team, and we didn't have the time or budget to focus significant amounts of effort on developing a new in-house system or buying something more robust.  Business leadership had demands for new innovations but even small changes to interfaces frustrated and slowed the productivity of our high-throughput data entry and data analyst workers.

I applied the 'ports and adapters' or 'hexagonal' architecture design pattern to solve this problem.  I didn't have to make time for a separate project because I was able to do the decoupling as a part of our already prioritized work.  Any time I worked on a story I made sure to decouple what I was working on from the third-party system so that it could integrate via an abstraction first.  Over time I did this at all integration points with the third-party system.  Eventually, all our databases and our front ends were no longer tightly coupled with that architecture.  I was able to do this without changing anything about the interface or user experience.

From there, I was able to steadily start implementing in-house improvements or even replacements for functionality we were previously relying on the third-party software for.  When the time came to update interfaces, I was able to do so by working closely with the users so they weren't frustrated with the changes, and we had much more ability to customize the experiences how they wanted because I no longer had to exclusively use the interfaces provided by the third-party solution.  I was also able to make many complex business workflows more reliable by caching data in our databases instead of relying on the in-memory state machine the third-party tool was using, so that business processes could be recovered where they were left off even in the event the third-party infrastructure had went down.

This eventually began to look like an event-driven architecture due to a few things that developed organically from this design pattern.  The most important was the practice of using a simple event object across all integration points that at the highest level was a SOAP Envelope that met all the schema and security requirements for the legacy software and other systems that communicated using SOAP.  Part of this schema contained a place for passing JSON data in a string.  From there central configuration could be used to determine which parts of the architecture would send or receive SOAP and which would send or receive REST.  In either case, the JSON was the same.  Then it was simply up to each component to know what it expected to parse from the JSON.  When components responded to one another they could return specific parsing error messages, already on the expected event response model, to allow the sender to make any needed corrections to their implementation.  

The other factor that made the system very event-driven was enhancing and replacing the less reliable in-memory message queuing mechanism built into the legacy software with an in-house MsMQ queue and backing up process state at important checkpoints using MsSQL.  This meant that events were no longer lost and even in a complete failure of the queue, new events could be initialized that would carry on the work just as it had been left.
